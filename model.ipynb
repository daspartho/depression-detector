{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece datasets"
      ],
      "metadata": {
        "id": "qr4fc7Cc5WUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"hugginglearners/reddit-depression-cleaned\", split='train')\n",
        "\n",
        "ds = ds.rename_column(\"clean_text\", \"text\")\n",
        "ds = ds.rename_column(\"is_depression\", \"label\")\n",
        "\n",
        "ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
        "\n",
        "ds"
      ],
      "metadata": {
        "id": "PJvW6n-M5Zyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok_func(x): return tokenizer(x[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "tok_ds = ds.map(tok_func, batched=True)\n",
        "tok_ds"
      ],
      "metadata": {
        "id": "KiiWvDzG5cGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "bs = 32\n",
        "epochs = 3\n",
        "lr = 1e-5\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True, \n",
        "                         evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, \n",
        "                         num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "trainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['test'], \n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "3oVw9WNL5eAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "label_map = {\n",
        "    'LABEL_0':\"You're doing great!\",\n",
        "    'LABEL_1':\"You need a hug? (っ◕‿◕)っ\",\n",
        "}\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "text = \"I have depression of epic proportions.\"\n",
        "print(label_map[pipe(text)[0]['label']])"
      ],
      "metadata": {
        "id": "YXhKUV4v5fjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4fb70c-d12a-4a74-e120-61c01910f7db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You need a hug? (っ◕‿◕)っ\n"
          ]
        }
      ]
    }
  ]
}